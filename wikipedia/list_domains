#! /usr/bin/env bash
# Info: Dumps of all external links in en, no and nn wikipedias 
# Download: 
SOURCE_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SOURCE_DIR"
source config.sh

ENWIKI="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-externallinks.sql.gz"
NOWIKI="https://dumps.wikimedia.org/nowiki/latest/nowiki-latest-externallinks.sql.gz"
NNWIKI="https://dumps.wikimedia.org/nnwiki/latest/nnwiki-latest-externallinks.sql.gz"

function dw {
    BN="$(basename "$1")"
    if [ ! -e "$BN" ]; then
        echo "[INFO] Fail to find $BN, downloading" 1>&2;
        wget -4 --timestamping "$1";
    else
        echo "[INFO] Found $BN" 1>&2;
    fi
}

dw "$ENWIKI"
dw "$NOWIKI"
dw "$NNWIKI"

function ed {
	BN="$(basename "$1")"
	echo >&2 "[INFO] Extracting $BN"
	gzip --stdout -d "$BN" | sed -e 's/),(/\n/g' | grep -a -F '.no' | "$MCN_TOOLS/urldecode" 3 > "$BN.list.tmp"
}

YMD="$(date +%F)"
COUNT="$(find . -name "$YM-$DOMAINS" 2>/dev/null | wc -l)";
if [ "$COUNT" -eq "0" ] || [ "x${1:-}" = "x--update-list" ]; then

  ed "$ENWIKI"
  ed "$NOWIKI"
  ed "$NNWIKI"

  echo >&2 "[INFO] Extracting domains. This may take a LONG time."
  find . -name '*.list.tmp' -exec cat {} \; | sed -e 's/.html.no/.html/g' | "$MCN_TOOLS/default_extract" > "_tmp.list"
  mv output/* old/ || true
  cat old/* "_tmp.list" | "$MCN_TOOLS/default_extract" > "$YMD-$DOMAINS"
  mv "$YMD-$DOMAINS" output/
  rm "_tmp.list"
fi
